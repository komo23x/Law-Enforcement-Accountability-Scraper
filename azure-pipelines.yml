# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

variables:
- name: DATETIME
  value: 01-01-1970@00:00:00

trigger: none
pr: none

schedules:
- cron: "0 16 * * *" # Run at 9AM PST daily
  displayName: "Daily Crawl"
  branches:
    include: 
    - master
  always: true

pool:
  vmImage: 'ubuntu-latest'

steps:
- task: UsePythonVersion@0
  inputs:
    versionSpec: '3.x'
    addToPath: true
    architecture: 'x64'
  displayName: 'Set Python version to Python 3'
- script: |
      echo "##vso[task.setvariable variable=DATETIME]$(date +%F@%H_%M_%S)"
      sudo apt install python3-pip
      pip3 install requests
      pip3 install beautifulsoup4
      python3 WS1_Web_Crawler_script.py
  displayName: 'Run crawler python script'
- task: CopyFiles@2
  inputs:
    SourceFolder: '$(System.DefaultWorkingDirectory)'
    Contents: 'ws1_web_crawler_results.csv'
    TargetFolder: '$(Build.ArtifactStagingDirectory)'
    OverWrite: true
  displayName: 'Copy CSV to staging directory'
- publish: '$(Build.ArtifactStagingDirectory)/ws1_web_crawler_results.csv'
  artifact: 'results'
  displayName: 'Copy build artifact'
- task: GitHubRelease@1
  inputs:
    gitHubConnection: 'Maxwell-McKee'
    repositoryName: '$(Build.Repository.Name)'
    action: 'create'
    target: '$(Build.SourceVersion)'
    tagSource: 'userSpecifiedTag'
    tag: '$(DATETIME)'
    title: 'Daily Crawl ($(DATETIME))'
    assets: '$(Build.ArtifactStagingDirectory)/ws1_web_crawler_results.csv'
    addChangeLog: false